HOST='0.0.0.0'
PORT=3000

# MODEL RELATED
MODEL='llama3.2-vision' # Be sure to use the correct model name - Set USE_LOCAL accordingly
USE_LOCAL=true          # Set to true if you want to use a local model

## REMOTE - OpenAI
OPENAI_API_KEY='<your-openai-api-key>'

## LOCAL - Ollama
OLLAMA_HOST='http://0.0.0.0'
OLLAMA_PORT=11434

# LANGCHAIN
LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
LANGCHAIN_API_KEY="<your-api-key-here>"
LANGCHAIN_PROJECT="<your-project-name-here>"